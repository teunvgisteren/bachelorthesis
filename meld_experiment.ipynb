{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb03d631-b6ea-47fd-9c2f-e8a82cccfcfc",
   "metadata": {},
   "source": [
    "### Experiments From \"Filtered Feelings: Investigating Frequency Filters in Speech Emotion Recognition Models\"\n",
    "Created by: Teun van Gisteren (s1055104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e178635-bfe1-4531-95fc-5869566eb703",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e23b7-43df-4042-a2bd-e2ee8e157a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell might return an \"TypeError: 'type' object is not subscriptable\" error. If you run the cell again it should work.\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import butter, lfilter, filtfilt, freqz\n",
    "from os import mkdir\n",
    "from os import makedirs\n",
    "from os.path import isdir\n",
    "import shutil\n",
    "\n",
    "from speechbrain.inference.interfaces import foreign_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d47725-4976-46b5-9d85-b7de7aca5d35",
   "metadata": {},
   "source": [
    "# 5.9 Testing on the MELD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eedf95-95b6-4040-88aa-a8855f3dab90",
   "metadata": {},
   "source": [
    "## MELD Data Handling & Preprocessing\n",
    "Note: The MELD dataset as used here was already resampled to 16000 Hz to bring it in line with the IEMOCAP dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2854fc-407d-44d9-b416-8b93321d901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each split, the name of the split, where the .wav files are located for that split, and where the .csv file with the emotion labels is for that split\n",
    "dev_split = {\"name\": \"dev\", \"directory\": \"...\\\\dev\", \"csv\": \"...\\\\dev_sent_emo.csv\"}\n",
    "test_split = {\"name\": \"test\", \"directory\": \"...\\\\test\", \"csv\": \"...\\\\test_sent_emo.csv\"}\n",
    "train_split = {\"name\": \"train\", \"directory\": \"...\\\\train\", \"csv\": \"...\\\\train.tar\\\\train_sent_emo.csv\"}\n",
    "\n",
    "results_location = None # Where you want the results .csv files to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e3107-02c4-4c8c-a589-22313f03f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MELD_emotions(split):\n",
    "    emotions = {}\n",
    "    # List of all emotions to be gotten from the list\n",
    "    possible_emotions = ['anger', 'joy', 'neutral', 'sadness']\n",
    "    # Necessary because of different label used in different datasets\n",
    "    emotion_map = {'anger': 'ang', 'joy': 'hap', 'neutral': 'neu', 'sadness': 'sad'}\n",
    "    \n",
    "    with open(split[\"csv\"], mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if(row['Emotion'] in possible_emotions):\n",
    "                emotions[(row[\"Dialogue_ID\"], row[\"Utterance_ID\"])] = emotion_map[row['Emotion']]\n",
    "                \n",
    "    return emotions\n",
    "\n",
    "def get_MELD_split_files(split):\n",
    "    # Get all files from the audio directory\n",
    "    files = os.listdir(split[\"directory\"])\n",
    "    # Filter only .wav files\n",
    "    wav_files = [file for file in files if file.endswith('.wav')]\n",
    "    \n",
    "    return wav_files\n",
    "\n",
    "def filter_appropriate_files(emotions, files):\n",
    "    # Pattern for finding all numbers in a string\n",
    "    pattern = r'\\d+'\n",
    "\n",
    "    # Iterate over the file list and filter out files not in the array\n",
    "    filtered_files = []\n",
    "    for filename in files:\n",
    "        # Extract Dialogue_ID and Utterance_ID from the file name\n",
    "        numbers = re.findall(pattern, filename)        \n",
    "        dialogue_id = numbers[0]\n",
    "        utterance_id = numbers[1]\n",
    "        # Check if the extracted pair exists in the dictionary keys\n",
    "        if (dialogue_id, utterance_id) in emotions.keys():\n",
    "            filtered_files.append(filename)\n",
    "\n",
    "    return filtered_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24ec58-ee8d-4d9a-b211-5fd89daf19f5",
   "metadata": {},
   "source": [
    "## CSV Export Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5922d3-2484-416d-a29c-7b978de4cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes a output path, file name, and results array and outputs the results array in CSV format to the output path in file name\n",
    "def results_to_csv(output_path, name, results):\n",
    "    # Get the absolute path to the new CSV file\n",
    "\n",
    "    # Create output path if it does not exist\n",
    "    if not os.path.exists(output_path):\n",
    "       os.makedirs(output_path)\n",
    "\n",
    "    csv_path = os.path.abspath(f\"{output_path}\\\\{name}.csv\")\n",
    "\n",
    "    with open(csv_path, mode='w', newline='', encoding='utf-8') as new_file:\n",
    "        fieldnames = [\"Split\", \"Dialogue_ID\", \"Utterance_ID\", \"Emotion\", \"Emotion_Guess\"]\n",
    "        writer = csv.DictWriter(new_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cd774-38bd-4e1e-8fba-b7e146ff94ca",
   "metadata": {},
   "source": [
    "## Accuracy Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a6c56-e502-4635-bfc7-6fc7db195662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to calculate the accuracy of the results, with optional decimal number length parameter \n",
    "def calculate_accuracy_str(results, dec_len=10):\n",
    "    actual = [item[\"Emotion\"] for item in results]\n",
    "    predicted = [item[\"Emotion_Guess\"] for item in results]\n",
    "    score = accuracy_score(actual, predicted)\n",
    "    return f\"Accuracy: {score * 100:.{dec_len}f}%\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece625c-24d4-479f-816a-329a4ed66b1b",
   "metadata": {},
   "source": [
    "## Model Initilization and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de504562-9303-429d-b906-d4518c16a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier\n",
    "classifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", \n",
    "                           pymodule_file=\"custom_interface.py\", \n",
    "                           classname=\"CustomEncoderWav2vec2Classifier\", \n",
    "                           run_opts={\"device\":\"cuda\"}) # Run on CUDA if you have a capable GPU. Remove parameter if you do not.\n",
    "\n",
    "# Function that takes a session id, a dictionary of files to the emotions expressed in those files, the files themselves\n",
    "# and a boolean for if the files are filtered, this is due to how the files are named.\n",
    "def run_classifier(split, emotions, files, filtered=False):\n",
    "    results = []\n",
    "    \n",
    "    # Count iterations for a nicer display\n",
    "    iterations = 1\n",
    "\n",
    "    # Process each file\n",
    "    for wav_file in files:\n",
    "        # Skip the file causing memory issues\n",
    "        if \"dia38_utt4\" in wav_file:\n",
    "            continue\n",
    "            \n",
    "        # Show progress text\n",
    "        progress = iterations/len(files)*100\n",
    "        print(\"\\rProcessing: {}, Progress: {:.2f}%\".format(wav_file, progress), end=\"\")\n",
    "\n",
    "        # Get file path of the .wav file\n",
    "        if filtered:\n",
    "            folder_name = wav_file.rsplit(\"_\", 3)[0]\n",
    "        else:\n",
    "            file_path = os.path.join(split[\"directory\"], wav_file)\n",
    "        \n",
    "        # Classify\n",
    "        out_prob, score, index, text_lab = classifier.classify_file(file_path)\n",
    "\n",
    "        # Pattern for finding all numbers in a string\n",
    "        pattern = r'\\d+'\n",
    "        # Extract Dialogue_ID and Utterance_ID from the file name\n",
    "        numbers = re.findall(pattern, wav_file)        \n",
    "        dialogue_id = numbers[0]\n",
    "        utterance_id = numbers[1]\n",
    "        \n",
    "        # Append data to results list\n",
    "        results.append({\n",
    "                \"Split\": split[\"name\"],\n",
    "                \"Dialogue_ID\": dialogue_id,\n",
    "                \"Utterance_ID\": utterance_id, \n",
    "                \"Emotion\": emotions[(dialogue_id, utterance_id)],\n",
    "                \"Emotion_Guess\": text_lab[0]\n",
    "            })\n",
    "\n",
    "        iterations += 1\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48704955-8509-4967-a884-310fd3e33c57",
   "metadata": {},
   "source": [
    "## Baseline Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf58a3d-8d9a-4d90-a0cd-3fe28f05ff70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_splits = [dev_split, test_split, train_split]\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "for split in all_splits:\n",
    "    print(f\"Split: {split['name']}\")\n",
    "    \n",
    "    # Get the appropriate files and corresponding emotional labels\n",
    "    emotions = get_MELD_emotions(split)\n",
    "    files = filter_appropriate_files(emotions, get_MELD_split_files(split))\n",
    "\n",
    "    # Run the classifier\n",
    "    results = run_classifier(split, emotions, files)\n",
    "\n",
    "    # Combine results with other data\n",
    "    combined_results = combined_results + results\n",
    "    \n",
    "     # Display classification stats per session\n",
    "    print(\"\\n\", calculate_accuracy_str(results, 2))\n",
    "    \n",
    "    # Print a nice line between splits\n",
    "    term_size = os.get_terminal_size()\n",
    "    print('=' * term_size.columns)\n",
    "    \n",
    "print(\"Overall\")\n",
    "print(calculate_accuracy_str(combined_results, 2))\n",
    "\n",
    "results_to_csv(results_location, \"MELD_Base\", combined_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013f3a9-63a2-4401-9af4-0cda10ab97b9",
   "metadata": {},
   "source": [
    "### Audio Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b611fee4-dc0f-46bb-a74a-c92e39abb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is a modified version of the code from this stack overflow post:\n",
    "# https://stackoverflow.com/questions/21871834/adding-effects-to-make-voice-sound-like-it-s-over-a-telephone\n",
    "\n",
    "# Creates a Butterworth filter from the low and high boundaries with a given order\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "# Applies the Butterworth filter \n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def bandpass_filter(buffer, lowcut, highcut, FRAME_RATE):\n",
    "    return butter_bandpass_filter(buffer, lowcut, highcut, FRAME_RATE, order=3)\n",
    "\n",
    "# Function that takes a list of files in the input path, filters them with the given low and high boundaries and outputs the files to the output path\n",
    "def filter_audio(low, high, sample, input_path, output_path, files):\n",
    "    if isdir(output_path) == False:\n",
    "        makedirs(output_path)\n",
    "    else:\n",
    "        shutil.rmtree(output_path)\n",
    "        makedirs(output_path)\n",
    "    \n",
    "    for file in files:\n",
    "        samplerate, data = wavfile.read(input_path + \"\\\\\" + file)\n",
    "        assert samplerate == sample\n",
    "        filtered = np.apply_along_axis(bandpass_filter, 0, data, lowcut=low, highcut=high, FRAME_RATE=sample).astype('int16')\n",
    "        wavfile.write(os.path.join(output_path, f'{file[:-4]}_l{low}_h{high}.wav'), samplerate, filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0b9cf-0ac0-4c52-bb81-28edb9049fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_MELD_split_files(split, output_path, low, high):  \n",
    "    files = []\n",
    "    \n",
    "    files = os.listdir(split[\"directory\"])\n",
    "    #print(files)\n",
    "    wavfiles = [file for file in files if file.endswith('.wav')]\n",
    "    filter_audio(low, high, 16000,\n",
    "                 split[\"directory\"], \n",
    "                 os.path.normpath(output_path), files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42ff3f-c0ed-4fac-a753-32a2db44293a",
   "metadata": {},
   "source": [
    "## Systematically Altering The Frequency Range on Audio Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82fc0e-8f1f-4e78-b2f0-f178a9161c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define new splits here to easily access the filtered files\n",
    "dev_split_filtered = {\"name\": \"dev\", \"directory\": \"...\\\\dev\", \"csv\": \"...\\\\dev_sent_emo.csv\"}\n",
    "test_split_filtered = {\"name\": \"test\", \"directory\": \"...\\\\test\", \"csv\": \"...\\\\test_sent_emo.csv\"}\n",
    "train_split_filtered = {\"name\": \"train\", \"directory\": \"...\\\\train\", \"csv\": \"...\\\\train.tar\\\\train_sent_emo.csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b27a8-5446-4e62-9a00-f0eea354fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematically_classify_filtered_files(low_bound, high_bound, step_size):\n",
    "    # Define the ranges for x (low) and y (high)\n",
    "    low_range = range(low_bound[0], low_bound[1], step_size)  \n",
    "    high_range = range(high_bound[0], high_bound[1], step_size)\n",
    "    \n",
    "    # Generate the coordinates making sure that the higher bound is not lower or equal to the lower bound\n",
    "    frequency_bands = [(x, y) for x in low_range for y in high_range if y > x]\n",
    "    \n",
    "    for step in frequency_bands:\n",
    "        # Filter the audio files\n",
    "        print(\"Filtering audio files...\")\n",
    "        filter_MELD_split_files(dev_split, dev_split_filtered[\"directory\"], step[0], step[1])\n",
    "        filter_MELD_split_files(train_split, train_split_filtered[\"directory\"], step[0], step[1])\n",
    "        filter_MELD_split_files(test_split, test_split_filtered[\"directory\"], step[0], step[1])\n",
    "        all_splits = [dev_split_filtered, test_split_filtered, train_split_filtered]\n",
    "        \n",
    "        combined_results = []\n",
    "        print(f\"Frequency Range between {step[0]} and {step[1]} Hz\")\n",
    "        # Loop through each session in the IEMOCAP database\n",
    "        for split in all_splits:\n",
    "            print(f\"Split: {split['name']}\")\n",
    "\n",
    "            emotions = get_MELD_emotions(split)\n",
    "            \n",
    "            # Get the appropriate files\n",
    "            files = filter_appropriate_files(emotions, get_MELD_split_files(split))\n",
    "            \n",
    "            # Run the classifier\n",
    "            results = run_classifier(split, emotions, files)\n",
    "            # Combine results with other data\n",
    "            combined_results = combined_results + results\n",
    "            print(\"\\n\")\n",
    "        results_to_csv(results_location, f\"MELD_Filtered_l{step[0]}_h{step[1]}\", combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2bd48c-28c8-4538-bfde-b1c89e98c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "systematically_classify_filtered_files([1, 4002], [1, 7999], 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
